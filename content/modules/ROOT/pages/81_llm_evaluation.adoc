# Evaluating Large Lanuage Models 

In this section we will cover how Large Language Models can be evaluated using LM-Eval framework.

This demo requires a separate RHOAI environment.

Start with provisioning a new demo environment as described in the Environment Provisioning section.

Make sure to choose `rhoai-eus-2.16-aws-gpu-trustyai` when running bootstrap.

[.bordershadow]
image::bootstrap-trustyai.png[]

Validate via ArgoCD that all components are deployed and synced. TrustyAI demo components are deployed via `ai-example-trusty-ai` application. It includes LLM model from https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct[]. Source code can be found under 'tenants/ai-example/trustyai-demo'. Minio deployment for object storage is done via 'ai-example-trustyai-demo-minio' ArgoCd application - source code is under 'tenants/ai-example/trustyai-demo-minio'.

'Qwen2.5-0.5B-Instruct' model requires Nvidia GPU - so it might take some time for GPU machine to be provisioned and model to start. You can monitor the process in OpenShift Console under `Compute -> Machines`. Model is deployed in `ai-example-trustyai-demo` namespace. Make sure that `qwen-instruct-predictor*` pod is up and running.

LLM Evaluation can be initiate by creating the `LMEvalJob` Custom Resource.

+
.lm-eval-job.yaml

[.console-input]
[source, yaml]
----
apiVersion: trustyai.opendatahub.io/v1alpha1
kind: LMEvalJob
metadata:
  name: evaljob
  namespace: ai-example-trustyai-demo
spec:
  model: local-completions
  taskList:
    taskNames:
      - arc_easy
  logSamples: true
  batchSize: '1'
  allowOnline: true
  allowCodeExecution: false
  outputs:
    pvcManaged:
      size: 5Gi
  modelArgs:
    - name: model
      value: qwen-instruct
    - name: base_url
      value: http://qwen-instruct-predictor:8080/v1/completions
    - name: num_concurrent
      value:  "1"
    - name: max_retries
      value:  "3"
    - name: tokenized_requests
      value: "False"
    - name: tokenizer
      value: Qwen/Qwen2.5-0.5B-Instruct
----

Copy the content of the yaml to a local file on your machine and apply it using the following command 

[SOURCE]
----
oc apply -f lm-eval-job.yaml
----

LM-Eval job will take some time to complete. The progress can be observed by monitoring `evaljob` pod in `ai-example-trustyai-demo` namespace.

Pod logs can be observed by running the following command:

[SOURCE]
----
oc logs -f pod/evaljob -n ai-example-trustyai-demo
----

If 'evaljob' pod fails to complete - this might be due to a known issue with TrustyAI operator not allowing online connections by default. Run the following commands to allow online connections:

[SOURCE]
----
oc patch configmap trustyai-service-operator-config -n redhat-ods-applications \
--type merge -p '{"data":{"lmes-allow-online":"true","lmes-allow-code-execution":"true"}}'

oc rollout restart deployment trustyai-service-operator-controller-manager -n redhat-ods-applications
----

Wait for couple minutes and then re-apply the LM-Eval job:

[SOURCE]
----
oc delete LMEvalJob/evaljob -n ai-example-trustyai-demo

oc apply -f lm-eval-job.yaml
----

Once LM-Eval job completes - the evaluation results can be retrieved by runnign the following command:

[SOURCE]
----
oc get lmevaljobs.trustyai.opendatahub.io evaljob -n ai-example-trustyai-demo \
  -o template --template={{.status.results}} | jq '.results'
{
  "arc_easy": {
    "alias": "arc_easy",
    "acc,none": 0.6561447811447811,
    "acc_stderr,none": 0.009746660584852454,
    "acc_norm,none": 0.5925925925925926,
    "acc_norm_stderr,none": 0.010082326627832872
  }
}
----

Results show accuracy scores of 0.6561447811447811 and 0.5925925925925926 with stderr of 0.009746660584852454 and 0.010082326627832872
